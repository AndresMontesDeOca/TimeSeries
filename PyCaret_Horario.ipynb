{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0baf38b",
      "metadata": {},
      "source": [
        "# PyCaret (usando datos por Hora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c9c30b4b",
      "metadata": {
        "noteable": {
          "output_collection_id": "99e967c4-2723-47ae-b49d-7fd61988eb96"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T04:22:38.118396+00:00",
          "start_time": "2023-12-06T04:21:24.819065+00:00"
        }
      },
      "outputs": [],
      "source": "# Agregamos Prophet a PyCaret, hay que reiniciar el Kernell despues\n!pip install prophet\n!pip install --pre pycaret\n!pip install pingouin\n!pip install openpyxl\n\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom pycaret.time_series import TSForecastingExperiment\nfrom matplotlib import pyplot as plt\n\n# Configuracion para graficas\nfig_kwargs = {\n#     \"renderer\": \"notebook\",\n    \"renderer\": \"png\",\n    \"width\": 800,\n    \"height\": 500,\n}\n\n# warnings.filterwarnings('ignore', category=ValueWarning)\nwarnings.filterwarnings('ignore')\n\n# Vamos a suprimir la notacion cientifica\npd.set_option(\"display.float_format\", lambda x:\"%.2f\" %x)\n\n# Usa una fuente que tengas instalada\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'DejaVu Sans'  \n"
    },
    {
      "cell_type": "markdown",
      "id": "3b4456bc",
      "metadata": {},
      "source": [
        "## Carga de los Datos por Dia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d0abd2d0",
      "metadata": {
        "scrolled": true,
        "noteable": {
          "output_collection_id": "43bf5252-656d-4874-8bfb-0f2a31ce17a2"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T04:22:39.604601+00:00",
          "start_time": "2023-12-06T04:22:38.126391+00:00"
        }
      },
      "outputs": [],
      "source": "# Cargamos Demanda por Dia\ndata = pd.read_excel('Data/data_hora.xlsx')\ndata['Hora'] = pd.to_datetime(data['Hora'])\ndata.set_index('Hora', inplace=True)\n\nprint(data.info())"
    },
    {
      "cell_type": "markdown",
      "id": "2c5dfd20",
      "metadata": {},
      "source": "* A diferencia de como hicimos en el TP#1, aqui cargamos los datos originales por hora en lugar de por dia\n* Serian unos 26000 registros en lugar de 1100"
    },
    {
      "cell_type": "markdown",
      "id": "642a413d",
      "metadata": {},
      "source": [
        "# EDA Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d67cc4df",
      "metadata": {
        "scrolled": true,
        "noteable": {
          "output_collection_id": "9a015f50-9c5e-4ff3-b0c1-1e8450b1ef49"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T04:32:39.461585+00:00",
          "start_time": "2023-12-06T04:22:39.686045+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# Create an EDA experiment ----\n",
        "eda = TSForecastingExperiment()\n",
        "\n",
        "# Con indice implicito. Tambien probar con indice explicito luego!!!!\n",
        "eda.setup(\n",
        "    data=data,\n",
        "    target='Demanda',\n",
        "#     index=data.index,\n",
        "    fh=31*24,\n",
        "    # Set defaults for the plots ----\n",
        "    fig_kwargs=fig_kwargs,\n",
        "    session_id=1515,\n",
        ")\n",
        "\n",
        "# Vemos \n",
        "eda.plot_model(plot=\"ccf\", fig_kwargs=fig_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a381021",
      "metadata": {},
      "source": "* No se observa ninguna correlacion entre los lags de las distintas Series, decidimos incluir las tres en el modelo"
    },
    {
      "cell_type": "markdown",
      "id": "3c57eddb",
      "metadata": {},
      "source": [
        "# AutoML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "56c39186",
      "metadata": {
        "scrolled": false,
        "noteable": {
          "output_collection_id": "5bd318c9-c3eb-4702-83ee-2e22e82a6150"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T04:40:26.633508+00:00",
          "start_time": "2023-12-06T04:32:40.073030+00:00"
        }
      },
      "outputs": [],
      "source": "# Instanciamos el Experimento y configuramos la Pipeline\nexp = TSForecastingExperiment()\n\n# enforce_exogenous=False --> Use multivariate forecasting when model supports it, else use univariate forecasting\nexp.setup(\n    data=data, target='Demanda', fh=31, enforce_exogenous=False,\n    fig_kwargs=fig_kwargs, session_id=1515\n)"
    },
    {
      "cell_type": "markdown",
      "id": "a4f64abf",
      "metadata": {},
      "source": [
        "* Instanciamos el experimento, reservando para test los ultimos 31 registros (Diciembre 2022)\n",
        "* Vemos como en la linea #24 ya se detecta la Estacionalidad Diara (ya que los datos son diarios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c46848",
      "metadata": {
        "scrolled": false,
        "noteable": {
          "output_collection_id": "8bc276eb-3657-4fe4-b8b6-7b7906a7e4d2"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T05:20:46.551098+00:00",
          "start_time": "2023-12-06T04:40:26.658381+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# Seteo con cuantos modelos me quiero quedar\n",
        "n = 3\n",
        "best_baseline_models = exp.compare_models(n_select=n, turbo=False,exclude=['tbats', 'bats', 'arima'])\n",
        "\n",
        "# Guardamos la grilla de metricas\n",
        "compare_metrics = exp.pull()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20454f5a",
      "metadata": {},
      "source": "* Vemos que el modelo Prohet arroja mejores resultados en todas las metricas.\n* Igualmente nos quedamos con los 3 mejores modelos para Ensamblarlos luego (Hubert, KNN, Random Forest)\n* Recordemos que el objetivo de esta notebook es ver lo sencillo que es aplicar esta libreia de AutoML llamada PyCaret, y no encontrar el modelo mas optimo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00a4802",
      "metadata": {
        "noteable": {
          "output_collection_id": "fa8fb37d-fbef-442f-ba92-c1829f3add0c"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:07:40.870221+00:00",
          "start_time": "2023-12-06T05:20:46.810132+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# # Tuneamos los Hyperparametros de los mejores modelos\n",
        "\n",
        "# Si solo nos quedamos con el mejor\n",
        "if n==1:\n",
        "    best_tuned_models = exp.tune_model(best_baseline_models)\n",
        "# Si nos quedamos con mas de uno, para hacer un blend\n",
        "else:\n",
        "    best_tuned_models = [exp.tune_model(model) for model in best_baseline_models]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765eaba1",
      "metadata": {},
      "source": [
        "* Tambien tiene una opcion para Auto Tunear los valores de los Hyperparametros, parecido a lo que vimos como GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdb54da",
      "metadata": {
        "scrolled": true,
        "noteable": {
          "output_collection_id": "e86d4881-3bdc-4d8e-800a-3dd1f4d2fdd0"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:07:41.082344+00:00",
          "start_time": "2023-12-06T07:07:41.071869+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# Aplicamos diferente peso a cada modelo\n",
        "top_model_metrics = compare_metrics.iloc[0:n]['MAE']\n",
        "display(top_model_metrics)\n",
        "\n",
        "# Si es un solo modelo, el peso es 1\n",
        "if n==1:\n",
        "    top_model_weights = pd.Series(1, name='MAE')\n",
        "# Si son varios modelos, se calcula el peso de cada uno\n",
        "else:\n",
        "    top_model_weights = 1 - top_model_metrics/top_model_metrics.sum()\n",
        "display(top_model_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19211b0",
      "metadata": {},
      "source": [
        "* Ponderamos a cada modelo segun su efectivdad, y elegimos el Mean Absolute Error como Metrica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6605cf",
      "metadata": {
        "scrolled": true,
        "noteable": {
          "output_collection_id": "8c7611e7-9991-42f4-98ae-63b8ed828ecd"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:08:53.694519+00:00",
          "start_time": "2023-12-06T07:07:41.272472+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# Combinamos los modelos con sus respectivos pesos \n",
        "if n==1:\n",
        "    blender = best_tuned_models\n",
        "else:\n",
        "    blender = exp.blend_models(best_tuned_models, method='mean', weights=top_model_weights.values.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29158491",
      "metadata": {},
      "source": [
        "* Vemos que tambien es sencillo hacer el Blend, y agregarle distintos pesos a cada modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bed81ed",
      "metadata": {
        "noteable": {
          "output_collection_id": "799bb663-86cc-4402-8359-1378574b58e6"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:09:01.247973+00:00",
          "start_time": "2023-12-06T07:08:53.780325+00:00"
        }
      },
      "outputs": [],
      "source": "# Predicted vs Reality\nexp.plot_model(estimator=blender, fig_kwargs=fig_kwargs)"
    },
    {
      "cell_type": "markdown",
      "id": "9d7e5e10",
      "metadata": {},
      "source": [
        "* Analizamos graficamente la efectividad de la prediccion en los datos de Test (Dicimebre 2022)\n",
        "* Esto lo seteamos en el parametro fh cuando creamos el Setup del modelo en las primeras lineas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aae441c",
      "metadata": {
        "scrolled": true,
        "noteable": {
          "output_collection_id": "827c7865-e603-49e0-afe6-76e767e1a92f"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:09:16.167150+00:00",
          "start_time": "2023-12-06T07:09:01.593189+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# NO FUNCIONA\n",
        "# Finalizamos el modelo, entrenando con todos los datos\n",
        "final_model = exp.finalize_model(blender)\n",
        "# print(exp.predict_model(final_model))\n",
        "\n",
        "# Forexast para los datos del futuro\n",
        "# exp.plot_model(final_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd6a536",
      "metadata": {},
      "source": [
        "* Finalizamos el modelo entrenandolo con todos los datos disponibles (Train + Test)\n",
        "* Predecimos los valores futuros todavia desconocidos (Enero 2023) y los visualizamos\n",
        "* Guardamos el modelo por si lo queremos usar en el futuro, sin tener que volver a crearlo desde cero"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89fee34e",
      "metadata": {},
      "source": [
        "# Analisis de Residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15ea92a",
      "metadata": {
        "noteable": {
          "output_collection_id": "5879e675-1996-47e7-9043-e967a24bbaf0"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:09:16.278829+00:00",
          "start_time": "2023-12-06T07:09:16.172462+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# Libreria importada del TP#1 para el analisis de residuos\n",
        "\n",
        "# Instalacion y carga\n",
        "# !pip install pingouin\n",
        "import pingouin as pg\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import jarque_bera\n",
        "from scipy.stats import shapiro\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "from pmdarima.arima.utils import ndiffs\n",
        "\n",
        "# Diferentes Tests de Normalidad\n",
        "def residuos_normality(residuos):\n",
        "  # Test Grafico\n",
        "  pg.qqplot(residuos, dist='norm')\n",
        "  plt.title('Normalidad de los Residuos')\n",
        "  plt.show()\n",
        "\n",
        "  # Test Analitico\n",
        "  jarquebera = jarque_bera(residuos)[1]\n",
        "  shapir = shapiro(residuos)[1]\n",
        "  print('Shapiro p-value:', np.round(shapir, 3))\n",
        "  print('Jarque-Bera p-value:', np.round(jarquebera, 3), '\\n')\n",
        "\n",
        "# Diferentes Tests de Autocorrelacion\n",
        "def residuos_autocorrelation(residuos):\n",
        "  # Test Grafico\n",
        "  plot_acf(residuos, lags=len(residuos)-1)\n",
        "  plt.title('Autocorrelacion de los Residuos')\n",
        "  plt.show()\n",
        "  # Tests Analiticos\n",
        "  dw = sm.stats.stattools.durbin_watson(residuos)\n",
        "  lb = sm.stats.acorr_ljungbox(residuos, lags=len(residuos)-1, return_df=True)\n",
        "  print('Durbin-Watson (~2 = No-Autocorrelation):', np.round(dw, 3))\n",
        "#   print('Ljung-Box p-value:', lb.lb_pvalue.values[3])\n",
        "\n",
        "# Test Grafico de Homocedasticidad, el Analitco no lo pude hacer funcionar\n",
        "def residuos_homocedasticity(residuos):\n",
        "  # Grafico\n",
        "  plt.scatter(range(len(residuos)), residuos)\n",
        "  plt.axhline(y=0, color='r', linestyle='--')\n",
        "  plt.xlabel('Tiempo')\n",
        "  plt.ylabel('Residuos')\n",
        "  plt.title('Homocedasticidad - Residuos a lo Largo del Tiempo')\n",
        "  plt.show()\n",
        "\n",
        "  # Analitico\n",
        "  X = np.arange(len(residuos))\n",
        "  X_with_const = sm.add_constant(X)\n",
        "  print('Breusch-Pagan p-value (H0: Homocedasticity):', het_breuschpagan(residuos, X_with_const)[1].round(3))\n",
        "\n",
        "# Llama a todos los test anteriores, mas la media\n",
        "def residuos_evaluation(residuos):\n",
        "  print('Media de los Residuos', np.round(residuos.mean(), 3))\n",
        "  residuos_normality(residuos)\n",
        "  residuos_autocorrelation(residuos)\n",
        "  residuos_homocedasticity(residuos)\n",
        "    \n",
        "    \n",
        "# Para obtener el d optimo a diferenciar\n",
        "def diferenciacion(y):\n",
        "  # Estimado de número de diferencias con ADF test:Dickey-Fuller\n",
        "  n_adf = ndiffs(y, test='adf')  # -> 0\n",
        "\n",
        "  # KPSS test (auto_arima default): Kwiatkowski-Phillips-Schmidt-Shin\n",
        "  n_kpss = ndiffs(y, test='kpss')  # -> 0\n",
        "\n",
        "  # PP test: Phillips-Perron\n",
        "  n_pp = ndiffs(y, test='pp')  # -> 0\n",
        "\n",
        "  print('Estimado de número de diferencias con KPSS test')\n",
        "  print(n_kpss)\n",
        "  print('Estimado de número de diferencias con ADF test')\n",
        "  print(n_adf)\n",
        "  print('Estimado de número de diferencias con PP test')\n",
        "  print(n_pp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d805f33",
      "metadata": {
        "scrolled": true,
        "noteable": {
          "output_collection_id": "99404c5c-becf-4aac-8b3f-3618548d086a"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:09:21.203396+00:00",
          "start_time": "2023-12-06T07:09:16.284414+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# PyCaret no trae los residuos del modelo, los calculo a mano\n",
        "residuos_blender = exp.predict_model(blender)['y_pred'] - exp.get_config('y_test')\n",
        "\n",
        "# # Llamo a la funcion creada del TP#1\n",
        "residuos_evaluation((residuos_blender))\n",
        "\n",
        "# Nombres de todos los modelos\n",
        "# best_baseline_models, best_tuned_models, blender, final_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d743d348",
      "metadata": {},
      "source": "* Utilizamos una Funcion creada en el TP#1 extendida para analizar los Residuos del Modelo Auto-Blend creado, y vemos:\n* Tanto los tests de Normalidad de los Residuos de Shapuro y de Jarque-Bera son medio border, la Normalidad de los mismos es cuestionable\n* El test de Homocedasticidad de Breusche-Pagan nos indica que no hay suficiente evidencia para rechazar la H0 de que los residuos son Homocedasticos. Sin embargo esto no se puede apreciar bien en el Test Grafico\n* El problema lo tenemos en la Autocorrelacion, la misma es indicada tanto en el test analitico de Durbin-Watson como en la grafica de PCAF (alta correlacion con el lag1). Recordemos que estas series **no estan diferenciadas**\n* Por todo lo anterior podemos decir que la combinacion de modelos tuneados no pareciera ser optima\n* Probamos el mismo Pipeline pero con los datos diferenciados, y tanto las metricas de performance como el analisis de diagnostico arrojaron mejores resultados. Compobamos que la diferenciacion es recomendad segun el test de KPSS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d8bcd2",
      "metadata": {
        "noteable": {
          "output_collection_id": "4db70ac4-ddce-486d-9362-433b1f8ed47e"
        },
        "ExecuteTime": {
          "end_time": "2023-12-06T07:09:27.229426+00:00",
          "start_time": "2023-12-06T07:09:26.271186+00:00"
        }
      },
      "outputs": [],
      "source": [
        "# Chequeo sobre Diferenciacion en la Serie Original de Demanda \n",
        "\n",
        "print('Para Demanda:')\n",
        "print(diferenciacion(data['Demanda']), '\\n')\n",
        "print('Para Viento:')\n",
        "print(diferenciacion(data['Viento']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985b78f0",
      "metadata": {},
      "source": "* Confirmamos con un test de KPSS que conviene diferenciar ambas series (Demanda y Viento) al menos una vez\n* Nos resulta raro que esto no haya sido captado automaticamente por PyCaret"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9 (Base)",
      "identifier": "base",
      "language": "python",
      "language_version": "3.9",
      "name": "python3.9"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "selected_hardware_size": "large",
    "kernel_info": {
      "name": "python3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}